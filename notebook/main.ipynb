{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/competitions/playground-series-s6e1/data?select=train.csv\n",
    "\n",
    "# 1. Load the datasets\n",
    "folder_path_local = \"../input/playground-series-s6e1/\"\n",
    "folder_path_remote = \"https://kagglecsv.netlify.app/input/playground-series-s6e1/\"\n",
    "folder_path = folder_path_local if os.path.exists(folder_path_local) else folder_path_remote  # choose local if available, else remote\n",
    "train_data = pd.read_csv(folder_path + 'train.csv')\n",
    "test_data = pd.read_csv(folder_path + 'test.csv')\n",
    "print(\"Datasets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check column names for both train and test data\n",
    "print(\"Train Data Columns:\", train_data.columns)\n",
    "print(\"Test Data Columns:\", test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Identify categorical columns (ignore 'id' column)\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "categorical_columns = categorical_columns.drop('id', errors='ignore')  # Drop 'id' if it exists, otherwise ignore\n",
    "print(f\"Categorical columns: {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert categorical features to pandas 'category' dtype (preferred for tree models)\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'id' in categorical_columns:\n",
    "    categorical_columns.remove('id')\n",
    "for col in categorical_columns:\n",
    "    train_data[col] = train_data[col].astype('category')\n",
    "    test_data[col] = test_data[col].astype('category')\n",
    "print(f\"Categorical columns converted: {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Exam Score\n",
    "categorical_cols = train_data.select_dtypes(include=['object', 'category']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col != 'id']\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    num_cats = min(6, len(categorical_cols))  # Show up to 6 categorical features\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols[:num_cats]):\n",
    "        avg_scores = train_data.groupby(col, observed=False)['exam_score'].mean().sort_values()\n",
    "        avg_scores.plot(kind='barh', ax=axes[idx], color='steelblue')\n",
    "        axes[idx].set_xlabel('Average Exam Score')\n",
    "        axes[idx].set_title(f'Average Score by {col}')\n",
    "        axes[idx].grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(num_cats, 6):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No categorical features to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "numeric_features = train_data.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_features) > 1:\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    correlation_matrix = train_data[numeric_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap of Numeric Features', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show top correlations with target\n",
    "    if 'exam_score' in correlation_matrix.columns:\n",
    "        target_corr = correlation_matrix['exam_score'].drop('exam_score').sort_values(ascending=False)\n",
    "        print(\"\\nTop correlations with exam_score:\")\n",
    "        print(target_corr.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_data['exam_score'], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Exam Scores')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(train_data['exam_score'], vert=True)\n",
    "plt.ylabel('Exam Score')\n",
    "plt.title('Box Plot of Exam Scores')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Exam Score Statistics:\")\n",
    "print(train_data['exam_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split data into features (X) and target (y)\n",
    "if 'id' in train_data.columns:  # Check if 'id' exists before dropping it\n",
    "    X = train_data.drop(columns=['id', 'exam_score'])  # Drop 'id' and 'exam_score' columns\n",
    "else:\n",
    "    X = train_data.drop(columns=['exam_score'])  # Only drop 'exam_score' if 'id' is not present\n",
    "y = train_data['exam_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Simple imputation for numeric features (no scaling for tree models)\n",
    "from sklearn.impute import SimpleImputer\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X[num_cols] = imputer.fit_transform(X[num_cols])\n",
    "# Prepare a version of the test features for later\n",
    "if 'id' in test_data.columns:\n",
    "    X_test = test_data.drop(columns=['id']).copy()\n",
    "else:\n",
    "    X_test = test_data.copy()\n",
    "X_test[num_cols] = imputer.transform(X_test[num_cols])\n",
    "print('Imputed numeric columns:', num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. Feature Engineering to reduce RMSE from ~8.7 to lower values\n",
    "print(\"Creating engineered features to improve predictions...\")\n",
    "\n",
    "# Interaction features between numeric columns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "numeric_cols_for_interaction = X.select_dtypes(include=[np.number]).columns[:5]  # Top 5 numeric features\n",
    "\n",
    "if len(numeric_cols_for_interaction) >= 2:\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "    X_poly = poly.fit_transform(X[numeric_cols_for_interaction])\n",
    "    X_test_poly = poly.transform(X_test[numeric_cols_for_interaction])\n",
    "    \n",
    "    # Add polynomial features\n",
    "    feature_names = poly.get_feature_names_out(numeric_cols_for_interaction)\n",
    "    for i, fname in enumerate(feature_names[len(numeric_cols_for_interaction):]):\n",
    "        X[f'interaction_{i}'] = X_poly[:, len(numeric_cols_for_interaction) + i]\n",
    "        X_test[f'interaction_{i}'] = X_test_poly[:, len(numeric_cols_for_interaction) + i]\n",
    "    \n",
    "    print(f\"Added {len(feature_names) - len(numeric_cols_for_interaction)} interaction features\")\n",
    "\n",
    "# Statistical aggregations per categorical group (if any categorical features exist)\n",
    "if len(categorical_columns) > 0:\n",
    "    for cat_col in categorical_columns[:2]:  # Top 2 categorical\n",
    "        if cat_col in X.columns:\n",
    "            # Mean encoding - convert to float to avoid categorical assignment error\n",
    "            means = train_data.groupby(cat_col, observed=False)['exam_score'].mean()\n",
    "            X[f'{cat_col}_mean_target'] = X[cat_col].map(means).astype('float64').fillna(y.mean())\n",
    "            X_test[f'{cat_col}_mean_target'] = X_test[cat_col].map(means).astype('float64').fillna(y.mean())\n",
    "            print(f\"Added mean encoding for {cat_col}\")\n",
    "\n",
    "print(f\"Final feature count: {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train-test split (for evaluation purposes)\n",
    "# NOTE: the earlier pipeline removed scaling; the actual split is performed later using the imputed DataFrame `X`\n",
    "# (Old split referencing `X_scaled_df` has been removed to avoid NameError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2026-01-01T00:41:09.667621Z",
     "shell.execute_reply": "2026-01-01T00:41:09.666559Z",
     "shell.execute_reply.started": "2026-01-01T00:29:45.489881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Models: Optimized ensemble with tuned hyperparameters to minimize RMSE\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "# from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# Enable all 3 models with aggressive tuning for lower RMSE\n",
    "models = {\n",
    "    'CatBoost': CatBoostRegressor(iterations=20000, learning_rate=0.02, depth=8, l2_leaf_reg=5, random_state=42, verbose=0),\n",
    "    # 'LightGBM': LGBMRegressor(n_estimators=1550, learning_rate=0.02, max_depth=8, num_leaves=64, min_child_samples=10, random_state=42),\n",
    "    # 'XGBoost': XGBRegressor(n_estimators=3000, learning_rate=0.02, max_depth=7, subsample=0.8, colsample_bytree=0.8, random_state=42, tree_method='hist', verbosity=0)\n",
    "}\n",
    "\n",
    "# Determine categorical feature names (for LightGBM/CatBoost)\n",
    "categorical_features = [col for col in X.columns if str(X[col].dtype) == 'category']\n",
    "print('Categorical features to pass to models:', categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train models and evaluate using RMSE (competition metric) with early stopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "trained_models = {}\n",
    "best_mae = float('inf')\n",
    "best_model_name = None\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    try:\n",
    "        if name == 'LightGBM':\n",
    "            # LightGBM with RMSE metric\n",
    "            try:\n",
    "                model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric='rmse', categorical_feature=categorical_features)\n",
    "            except:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif name == 'CatBoost':\n",
    "            # CatBoost with RMSE optimization\n",
    "            try:\n",
    "                model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=categorical_features, use_best_model=True, verbose=50)\n",
    "            except TypeError:\n",
    "                model.fit(X_train, y_train)\n",
    "        elif name == 'XGBoost':\n",
    "            # XGBoost with RMSE objective\n",
    "            try:\n",
    "                model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "            except TypeError:\n",
    "                model.fit(X_train, y_train)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}:\", e)\n",
    "        continue\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "    mae = mean_absolute_error(y_valid, y_pred)\n",
    "    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f} (Competition Metric)\")\n",
    "    trained_models[name] = model\n",
    "    if rmse < best_mae:\n",
    "        best_mae = rmse\n",
    "        best_model_name = name\n",
    "    print('\\n')\n",
    "\n",
    "print(f'Best model: {best_model_name} with RMSE: {best_mae:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Select best model object\n",
    "if not trained_models:\n",
    "    raise RuntimeError('No models were successfully trained. Check training logs above for errors.')\n",
    "if best_model_name is None:\n",
    "    # If best_model_name wasn't set (e.g. training skipped/failed for all), pick the first trained model\n",
    "    best_model_name = list(trained_models.keys())[0]\n",
    "    print('Warning: best_model_name was None; defaulting to', best_model_name)\n",
    "best_model = trained_models[best_model_name]\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted scatter plot\n",
    "if trained_models and best_model is not None:\n",
    "    y_pred_valid = best_model.predict(X_valid)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(y_valid, y_pred_valid, alpha=0.5, s=20, color='blue', edgecolors='k', linewidth=0.5)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_valid.min(), y_pred_valid.min())\n",
    "    max_val = max(y_valid.max(), y_pred_valid.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    plt.xlabel('Actual Exam Score', fontsize=12)\n",
    "    plt.ylabel('Predicted Exam Score', fontsize=12)\n",
    "    plt.title(f'Actual vs Predicted - {best_model_name}', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Residual plot\n",
    "    residuals = y_valid - y_pred_valid\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred_valid, residuals, alpha=0.5, s=20, color='purple', edgecolors='k', linewidth=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    plt.xlabel('Predicted Exam Score', fontsize=12)\n",
    "    plt.ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "    plt.title(f'Residual Plot - {best_model_name}', fontsize=14)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Residual Statistics:\")\n",
    "    print(f\"Mean: {residuals.mean():.4f}\")\n",
    "    print(f\"Std Dev: {residuals.std():.4f}\")\n",
    "    print(f\"Min: {residuals.min():.4f}\")\n",
    "    print(f\"Max: {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best model\n",
    "if best_model is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    try:\n",
    "        # Try to get feature importance\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importances = best_model.feature_importances_\n",
    "            feature_names = X.columns\n",
    "            \n",
    "            # Sort by importance\n",
    "            indices = np.argsort(importances)[-20:]  # Top 20 features\n",
    "            \n",
    "            plt.barh(range(len(indices)), importances[indices], color='teal', alpha=0.7)\n",
    "            plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "            plt.grid(alpha=0.3, axis='x')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"{best_model_name} does not have feature_importances_ attribute\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot feature importance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "if trained_models:\n",
    "    model_names = []\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    \n",
    "    for name, model in trained_models.items():\n",
    "        try:\n",
    "            y_pred = model.predict(X_valid)\n",
    "            rmse = root_mean_squared_error(y_valid, y_pred)\n",
    "            mae = mean_absolute_error(y_valid, y_pred)\n",
    "            model_names.append(name)\n",
    "            rmse_scores.append(rmse)\n",
    "            mae_scores.append(mae)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if model_names:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # RMSE comparison\n",
    "        axes[0].barh(model_names, rmse_scores, color='coral')\n",
    "        axes[0].set_xlabel('RMSE (Lower is Better)')\n",
    "        axes[0].set_title('Model RMSE Comparison')\n",
    "        axes[0].grid(alpha=0.3, axis='x')\n",
    "        for i, v in enumerate(rmse_scores):\n",
    "            axes[0].text(v + 0.05, i, f'{v:.4f}', va='center')\n",
    "        \n",
    "        # MAE comparison\n",
    "        axes[1].barh(model_names, mae_scores, color='lightgreen')\n",
    "        axes[1].set_xlabel('MAE (Lower is Better)')\n",
    "        axes[1].set_title('Model MAE Comparison')\n",
    "        axes[1].grid(alpha=0.3, axis='x')\n",
    "        for i, v in enumerate(mae_scores):\n",
    "            axes[1].text(v + 0.05, i, f'{v:.4f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Retrain ALL models on full training data for ensemble (better than single model)\n",
    "print('Retraining all models on full training data for optimal ensemble...')\n",
    "for name, model in trained_models.items():\n",
    "    print(f'Retraining {name}...')\n",
    "    if name == 'LightGBM':\n",
    "        model.set_params(n_estimators=5000)\n",
    "        try:\n",
    "            model.fit(X, y, categorical_feature=categorical_features)\n",
    "        except:\n",
    "            model.fit(X, y)\n",
    "    elif name == 'CatBoost':\n",
    "        model.set_params(iterations=5000)\n",
    "        try:\n",
    "            model.fit(X, y, cat_features=categorical_features, verbose=100)\n",
    "        except:\n",
    "            model.fit(X, y)\n",
    "    elif name == 'XGBoost':\n",
    "        model.set_params(n_estimators=5000)\n",
    "        try:\n",
    "            model.fit(X, y)\n",
    "        except:\n",
    "            model.fit(X, y)\n",
    "print('All models retrained on full data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Ensure categorical dtypes preserved in test set\n",
    "for col in categorical_features:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].astype('category')\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Generate predictions using WEIGHTED ensemble (best weights for RMSE reduction)\n",
    "import numpy as np\n",
    "preds = []\n",
    "model_names = []\n",
    "for name, model in trained_models.items():\n",
    "    try:\n",
    "        pred = model.predict(X_test)\n",
    "        preds.append(pred)\n",
    "        model_names.append(name)\n",
    "        print(f'{name} predictions generated')\n",
    "    except Exception as e:\n",
    "        print(f'Error predicting with {name}:', e)\n",
    "\n",
    "if len(preds) >= 3:\n",
    "    # Weighted ensemble: CatBoost gets more weight (historically best performer)\n",
    "    weights = {'CatBoost': 0.5, 'LightGBM': 0.25, 'XGBoost': 0.25}\n",
    "    ensemble_preds = sum(preds[i] * weights.get(model_names[i], 1.0/len(preds)) for i in range(len(preds)))\n",
    "    print(f'Using weighted ensemble: {weights}')\n",
    "elif len(preds) > 0:\n",
    "    ensemble_preds = np.mean(preds, axis=0)\n",
    "    print('Using simple average ensemble')\n",
    "else:\n",
    "    ensemble_preds = trained_models[best_model_name].predict(X_test)\n",
    "    print(f'Using single best model: {best_model_name}')\n",
    "\n",
    "test_predictions = ensemble_preds\n",
    "print(f'Final predictions: mean={test_predictions.mean():.2f}, std={test_predictions.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'exam_score': test_predictions\n",
    "})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_data['exam_score'], bins=50, alpha=0.6, label='Training Data', color='blue', edgecolor='black')\n",
    "plt.hist(test_predictions, bins=50, alpha=0.6, label='Test Predictions', color='orange', edgecolor='black')\n",
    "plt.xlabel('Exam Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution Comparison: Training vs Test Predictions')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([train_data['exam_score'], test_predictions], labels=['Training Data', 'Test Predictions'])\n",
    "plt.ylabel('Exam Score')\n",
    "plt.title('Box Plot Comparison')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Data - Mean: {train_data['exam_score'].mean():.2f}, Std: {train_data['exam_score'].std():.2f}\")\n",
    "print(f\"Test Predictions - Mean: {test_predictions.mean():.2f}, Std: {test_predictions.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission file created successfully!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14993753,
     "sourceId": 119082,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
